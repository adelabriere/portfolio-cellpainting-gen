{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b32e4c06",
   "metadata": {},
   "source": [
    "# Reconstruction of cellpainting image using ML\n",
    "\n",
    "This notebook aims at showing basic models and training and generation for Cell-Painting data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5192b964",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "import torch\n",
    "import json\n",
    "import os\n",
    "import sys\n",
    "import importlib\n",
    "import shutil\n",
    "import numpy as np \n",
    "import lightning as L\n",
    "\n",
    "from lightning.pytorch.utilities.model_summary import ModelSummary\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision.transforms.functional as F\n",
    "from torchvision.transforms import v2\n",
    "\n",
    "plt.rcParams[\"savefig.bbox\"] = 'tight'\n",
    "\n",
    "# Adding the package in loading path\n",
    "sys.path.extend([\"../\"])\n",
    "\n",
    "import gencellpainting as gc\n",
    "from gencellpainting.utils.dataset import WGANCriticDataset, CellPaintingDatasetInMemory\n",
    "from gencellpainting.model import *\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93d65d05",
   "metadata": {},
   "source": [
    "### Paths\n",
    "\n",
    "We define the paths to the dataset and to store the trained models, the training tensorboard logs as well as the image for evaluations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7286ce30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path of the data to update if you want to run this script\n",
    "PATH_DATASET = \"/mnt/c/Users/alexi/Documents/data/images/cellpainting/cpg0016-jump/data/jump_64px_uint8.pt\"\n",
    "\n",
    "PATH_ROOT = os.path.abspath(\"../..\")\n",
    "\n",
    "# Path of optimized parameters\n",
    "PATH_OPTIM = os.path.join(PATH_ROOT,\"data\",\"optim\")\n",
    "\n",
    "# Path of the output of the model\n",
    "PATH_OUTPUT = os.path.abspath(os.path.join(PATH_ROOT,\"output\"))\n",
    "PATH_MODELS = os.path.join(PATH_OUTPUT,\"models\")\n",
    "PATH_TSB_LOGS = os.path.join(PATH_OUTPUT,\"tensorboard_logs\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b381cb8d",
   "metadata": {},
   "source": [
    "### Parameters\n",
    "\n",
    "High-levels parameters, which will be used during training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "638132df",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "MAX_EPOCHS = 100\n",
    "TEST_FRACTION = 0.2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "993f7aa3",
   "metadata": {},
   "source": [
    "### Loading the data\n",
    "\n",
    "The data as already been preprocessed into tensor of dimension N, C, H, W where:\n",
    "* __N__ is the number of example\n",
    "* __C__ is the rnumber of channel: in this case 5\n",
    "* __H__ is the heigth of the image in this case after resizing 64.\n",
    "* __W__ is the wifdth of the image in this case after resizing 64.\n",
    "This tensor is directly passed to the dataset we constructed. It could also be used directly with a _TensorDataset_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8be1a6fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = CellPaintingDatasetInMemory(tensor=torch.load(PATH_DATASET))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b63e068",
   "metadata": {},
   "source": [
    "We save the image dimension to use as parameters when training the networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71bab67b",
   "metadata": {},
   "outputs": [],
   "source": [
    "image = ds[1000]\n",
    "C,H,W = image.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "199479fe",
   "metadata": {},
   "source": [
    "We can define some utility functions to visualize the images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "005af206",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization fucntion take from  https://docs.pytorch.org/vision/main/auto_examples/others/plot_visualization_utils.html#sphx-glr-auto-examples-others-plot-visualization-utils-py\n",
    "def show(imgs):\n",
    "    if not isinstance(imgs, list):\n",
    "        imgs = [imgs]\n",
    "    fig, axs = plt.subplots(ncols=len(imgs), squeeze=False)\n",
    "    for i, img in enumerate(imgs):\n",
    "        img = img.detach()\n",
    "        img = F.to_pil_image(img)\n",
    "        axs[0, i].imshow(np.asarray(img))\n",
    "        axs[0, i].set(xticklabels=[], yticklabels=[], xticks=[], yticks=[])\n",
    "\n",
    "def plot_cellpainting_image(image, nrow=3):\n",
    "    imgs = list(torch.split(image,1,dim=0))\n",
    "    print([x.shape for x in imgs])\n",
    "    grid = torchvision.utils.make_grid(imgs,nrow = nrow)\n",
    "    show(grid)\n",
    "\n",
    "plot_cellpainting_image(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97c0ce41",
   "metadata": {},
   "source": [
    "In order to diversify the amount of image, we add a set of transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a12faaff",
   "metadata": {},
   "outputs": [],
   "source": [
    "transforms = v2.Compose([\n",
    "    v2.RandomHorizontalFlip(p = 0.5),\n",
    "    v2.RandomVerticalFlip(p=0.5),\n",
    "    v2.ToDtype(torch.float32, scale=True) # Tensor values [0, 255] -> [0.0, 1.0]\n",
    "])\n",
    "# We pass it directly to the datasets transfroms for convenience.\n",
    "ds.transform = transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46ba6d4b",
   "metadata": {},
   "source": [
    "We can split the dataset into a train and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a2db3c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_train, ds_test = torch.utils.data.random_split(ds, [1-TEST_FRACTION,TEST_FRACTION])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7df748c3",
   "metadata": {},
   "source": [
    "No in order to perform the learning we need to create batches using a _DataLoader_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29cdfb08",
   "metadata": {},
   "outputs": [],
   "source": [
    "dl_train = torch.utils.data.DataLoader(ds_train,batch_size=BATCH_SIZE,shuffle=True,num_workers=4)\n",
    "dl_test = torch.utils.data.DataLoader(ds_test,batch_size=BATCH_SIZE,shuffle=False,num_workers=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdf676f8",
   "metadata": {},
   "source": [
    "### Logging\n",
    "\n",
    "We log the training using tensorboard, storing some diagnosis metrics as well as sampling some images at each trainig steps. You can run tensorboard `tensorboard --logdir {PATH_TSB_LOGS}` using the path defined at the beginning of this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1460c712",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We erase the logs if they exists\n",
    "if os.path.exists(PATH_TSB_LOGS):\n",
    "    shutil.rmtree(PATH_TSB_LOGS)\n",
    "print(\"Logging for tensorboard in '{}'\".format(PATH_TSB_LOGS))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5a435e9",
   "metadata": {},
   "source": [
    "# Training\n",
    "\n",
    "We first retrieve the best parameters sets after the grid search for the VAE and WGAN-GP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51bd9723",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_best_params(model_name):\n",
    "    OPTIM_NAME = model_name + \"_hpar_optim\"\n",
    "    json_path = os.path.join(PATH_OPTIM,OPTIM_NAME,\"best_parameters_\"+OPTIM_NAME+\".json\")\n",
    "    with open(json_path,\"r\") as f:\n",
    "        params = json.load(f)\n",
    "    return params\n",
    "\n",
    "params_VAE = load_best_params(\"VAE\")\n",
    "params_WGANGP = load_best_params(\"WGANGP\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ce90f0a",
   "metadata": {},
   "source": [
    "\n",
    "## $\\beta$-VAE \n",
    "We first train the $\\beta$-VAE using the optimized parameters. We update the monitoring parameters to output images during training at every epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15f6e50e",
   "metadata": {},
   "outputs": [],
   "source": [
    "params_VAE[\"epoch_monitoring_interval\"] = 1\n",
    "params_VAE[\"n_images_monitoring\"] = 6\n",
    "params_VAE[\"latent_dim\"] = int(params_VAE[\"latent_dim\"])\n",
    "params_VAE[\"network_capacity\"] = int(params_VAE[\"network_capacity\"])\n",
    "print(params_VAE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e48773f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_vae = VAE(**params_VAE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d913dafa",
   "metadata": {},
   "source": [
    "A good practice is to evaluate if:\n",
    "\n",
    "1. The model can actually run.\n",
    "2. We don t have a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c145f675",
   "metadata": {},
   "outputs": [],
   "source": [
    "ModelSummary(model_vae)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04574eee",
   "metadata": {},
   "source": [
    "We configure the trainer, with an early stopping case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82a6ac73",
   "metadata": {},
   "outputs": [],
   "source": [
    "NAME_MODEL = \"VAE\"\n",
    "plogs = os.path.join(PATH_TSB_LOGS,NAME_MODEL)\n",
    "# Deleting the model log folder if it already exists for clarity\n",
    "if os.path.isdir(plogs):\n",
    "    shutil.rmtree(plogs)\n",
    "tb_logger = L.pytorch.loggers.TensorBoardLogger(save_dir = PATH_TSB_LOGS, name=NAME_MODEL)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "234af873",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = L.Trainer(max_epochs=100, accelerator=\"gpu\", devices=1, logger=tb_logger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "968c68db",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.fit(model_vae, train_dataloaders=dl_train, val_dataloaders=dl_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e33e863e",
   "metadata": {},
   "source": [
    "We save the model after training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8ef2f77",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_checkpoint(os.path.join(PATH_MODELS,NAME_MODEL))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5811fc73",
   "metadata": {},
   "outputs": [],
   "source": [
    "b1 = ds_test[10]\n",
    "b1 = b1[None,:,:,:]\n",
    "b1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce8fb50a",
   "metadata": {},
   "outputs": [],
   "source": [
    "y1 = model_vae.decoder(model_vae.encoder(b1).sample())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e33f9c38",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_cellpainting_image(b1.squeeze())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c51ca44d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_cellpainting_image(y1.squeeze())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc2e8b16",
   "metadata": {},
   "source": [
    "### Wasserstein GAN\n",
    "\n",
    "The Wasserstein GAN are a more stable version of GAN, and to be frank I am just curious of their performance. We first have to modify the dataset. A wasserstein GAN need _N_ samples to train the generator and _N x C_ sample to train the _C_ critics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaf4ddd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "NCRITICS = 5\n",
    "ds_W = WGANCriticDataset(ds,ncritic=NCRITICS)\n",
    "ds_train_W, ds_test_W = torch.utils.data.random_split(ds_W, [1-TEST_FRACTION,TEST_FRACTION])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df585b46",
   "metadata": {},
   "source": [
    "In order for the dataloader to know how to stitch a batch together we need to provide a specific `collate_fn` argument"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b6f2a8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function will return a tuple with 2 elements\n",
    "# 1. The images to process with the generator (B x C x H x W)\n",
    "# 2. The images to process with the discriminator for learning ( (B x NCRITIC) x C x H x W) \n",
    "def collate_wgan_batch(batch):\n",
    "    gen_imgs,disc_imgs = zip(*batch)\n",
    "    gen_imgs = torch.stack(gen_imgs)\n",
    "    disc_imgs = torch.stack([y for subbatch in disc_imgs for y in subbatch ])\n",
    "    return gen_imgs, disc_imgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cde5fa86",
   "metadata": {},
   "outputs": [],
   "source": [
    "dl_train_W = torch.utils.data.DataLoader(ds_train_W,batch_size=32,shuffle=True, collate_fn=collate_wgan_batch, num_workers=4)\n",
    "dl_test_W = torch.utils.data.DataLoader(ds_test_W,batch_size=32,shuffle=True, collate_fn=collate_wgan_batch, num_workers=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26ebac65",
   "metadata": {},
   "source": [
    "We can now test the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "734e66fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gencellpainting.model.WGAN as WGAN\n",
    "importlib.reload(WGAN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cbbf314",
   "metadata": {},
   "outputs": [],
   "source": [
    "params_WGANGP[\"epoch_monitoring_interval\"] = 1\n",
    "params_WGANGP[\"n_images_monitoring\"] = 3\n",
    "params_WGANGP[\"noise_dim\"] = int(params_WGANGP[\"noise_dim\"])\n",
    "params_WGANGP[\"network_capacity\"] = int(params_WGANGP[\"network_capacity\"])\n",
    "print(params_VAE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f000017",
   "metadata": {},
   "outputs": [],
   "source": [
    "wgan = WGAN.WGAN_GP(**params_WGANGP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f722f59c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ModelSummary(wgan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "419feec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "tb_logger = L.pytorch.loggers.TensorBoardLogger(save_dir = PATH_TSB_LOGS, name=\"WGAN_GP\")\n",
    "trainer_wg = L.Trainer(max_epochs=100,accelerator=\"gpu\",devices=1, logger=tb_logger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "861bbd35",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer_wg.fit(wgan,dl_train_W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b08b00b",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer_wg.save_checkpoint(os.path.join(PATH_MODELS,\"WGANGP\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f7508f4",
   "metadata": {},
   "source": [
    "### Diffusion based model\n",
    "\n",
    "This section presents the diffusion training process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af70e468",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gencellpainting.model.net.UNETdiffusion as UND\n",
    "importlib.reload(UND)\n",
    "import gencellpainting.model.diffusion as DIF\n",
    "importlib.reload(DIF)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e357dd8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "TIME_CHANNELS = 62\n",
    "NETWORK_CAPACITY = 32\n",
    "NSTEPS = 200\n",
    "NLAYERS = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c368703",
   "metadata": {},
   "source": [
    "We can now create the diffusion process using the UNET created"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88086a8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "diff_unet = UND.UNetDiffusionV2(ds.n_channels, ds.n_channels, time_channels = TIME_CHANNELS, network_capacity=NETWORK_CAPACITY, nlayers=NLAYERS)\n",
    "\n",
    "diffusion = DIF.DiffusionProcess(1,time_dim=TIME_CHANNELS,nsteps=NSTEPS,\\\n",
    "                                 model=diff_unet, include_time_emb=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a193bca",
   "metadata": {},
   "source": [
    "This version of diffusion is suited to generate images with values in [-1,1], our current dataset have value between [0,1]. We can create a new dataloader to rescale the image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "381b5e17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gaussian scaling\n",
    "ds.tensor.float().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cc660e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_diff_batch(batch):\n",
    "    gen_imgs = batch\n",
    "    gen_imgs = torch.stack(gen_imgs)\n",
    "    gen_imgs = gen_imgs * 2. - 1.\n",
    "    return gen_imgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4c59cb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "dl_train_diff = torch.utils.data.DataLoader(ds_train,batch_size=BATCH_SIZE,shuffle=True, collate_fn=collate_diff_batch, num_workers=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8c97235",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0303a49d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ModelSummary(diffusion)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b44f2067",
   "metadata": {},
   "source": [
    "We need to train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "456af555",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "NAME_MODEL = \"DIFFUNET\"\n",
    "plogs = os.path.join(PATH_TSB_LOGS,NAME_MODEL)\n",
    "if os.path.isdir(plogs):\n",
    "    shutil.rmtree(plogs)\n",
    "tb_logger = L.pytorch.loggers.TensorBoardLogger(save_dir = PATH_TSB_LOGS, name=NAME_MODEL)\n",
    "\n",
    "trainer_diff = L.Trainer(max_epochs=100,accelerator=\"gpu\",devices=1, logger=tb_logger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca53178e",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer_diff.fit(diffusion,dl_train_diff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86987c73",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer_diff.save_checkpoint(os.path.join(PATH_MODELS,NAME_MODEL))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "020b74ad",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e647b60b",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "imaging",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
