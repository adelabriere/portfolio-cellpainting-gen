{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "25738efd",
   "metadata": {},
   "source": [
    "### Model evaluation\n",
    "\n",
    "We evaluat ehte three models using different \n",
    "\n",
    "1. Pseudo FID: We compute the Frecht distance between the CLIP () embdeddings of the dataset and .... generated images.\n",
    "2. LPIP sdisnrtace\n",
    "\n",
    "We first load the necessary code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d92e2ebd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/adelabriere/miniconda3/envs/imaging/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torchvision\n",
    "import torch\n",
    "import json\n",
    "import os\n",
    "import sys\n",
    "import importlib\n",
    "import shutil\n",
    "import numpy as np \n",
    "import lightning as L\n",
    "import tqdm\n",
    "\n",
    "from lightning.pytorch.utilities.model_summary import ModelSummary\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision.transforms.functional as F\n",
    "from torchvision.transforms import v2\n",
    "\n",
    "plt.rcParams[\"savefig.bbox\"] = 'tight'\n",
    "\n",
    "# Adding the package in loading path\n",
    "sys.path.extend([\"../\"])\n",
    "\n",
    "import gencellpainting as gc\n",
    "from gencellpainting.utils.dataset import WGANCriticDataset, CellPaintingDatasetInMemory\n",
    "from gencellpainting.model import *\n",
    "from gencellpainting.evaluation.clip_fih import FrechetCLIPDistance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5f70e2a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path of the .pt fil\n",
    "PATH_DATASET = \"/mnt/c/Users/alexi/Documents/data/images/cellpainting/cpg0016-jump/data/jump_64px_uint8.pt\"\n",
    "# Path of saved models\n",
    "PATH_OUTPUT= os.path.join(os.path.abspath(\"../..\"),\"output\")\n",
    "PATH_MODEL = os.path.join(PATH_OUTPUT,\"models\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33196910",
   "metadata": {},
   "outputs": [],
   "source": [
    "NREPEAT = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef979b48",
   "metadata": {},
   "source": [
    "We first load the  full dataset, which will be used to compute the distribution of CLIP embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7609cffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = CellPaintingDatasetInMemory(tensor=torch.load(PATH_DATASET))\n",
    "ds = torch.utils.data.Subset(ds,range(5000))\n",
    "dl = torch.utils.data.DataLoader(ds, batch_size=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8176dd75",
   "metadata": {},
   "source": [
    "We load the pretrained models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dd917f60",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/adelabriere/miniconda3/envs/imaging/lib/python3.12/site-packages/lightning/pytorch/utilities/parsing.py:209: Attribute 'model' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['model'])`.\n"
     ]
    }
   ],
   "source": [
    "wgangp = WGAN_GP.load_from_checkpoint(os.path.join(PATH_MODEL,\"WGANGP\")).to(\"cuda\")\n",
    "diffusion = DiffusionProcess.load_from_checkpoint(os.path.join(PATH_MODEL,\"DIFFUNET\")).to(\"cuda\")\n",
    "vae = VAE.load_from_checkpoint(os.path.join(PATH_MODEL,\"VAE\")).to(\"cuda\")\n",
    "MODELS = {\n",
    "    \"VAE\":vae,\n",
    "    \"WGANGP\":wgangp,\n",
    "    \"Diff-UNet\":diffusion\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "200fc098",
   "metadata": {},
   "source": [
    "We define a function to update the FID."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "20dbd7ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_fih(dl, model):\n",
    "    with torch.no_grad():\n",
    "        clip_distance = FrechetCLIPDistance().to(\"cuda\")\n",
    "        total_samples = 0\n",
    "        for batch in tqdm.tqdm(dl):\n",
    "            B, _, _, _ = batch.size()\n",
    "            total_samples += B\n",
    "            batch = batch.float().to(\"cuda\")\n",
    "            clip_distance.update(batch, is_real=True)\n",
    "            fake_images = model.generate_images(batch=batch,n = B)\n",
    "            fake_images = fake_images.to(\"cuda\")\n",
    "            clip_distance.update(fake_images, is_real=False)\n",
    "        return clip_distance.compute(), total_samples\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "548f1628",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/79 [00:00<?, ?it/s]/home/adelabriere/miniconda3/envs/imaging/lib/python3.12/site-packages/torch/nn/modules/conv.py:549: UserWarning: Using padding='same' with even kernel lengths and odd dilation may require a zero-padded copy of the input be created (Triggered internally at /pytorch/aten/src/ATen/native/Convolution.cpp:1036.)\n",
      "  return F.conv2d(\n",
      "100%|██████████| 79/79 [00:41<00:00,  1.92it/s]\n",
      "100%|██████████| 79/79 [00:38<00:00,  2.03it/s]\n",
      "100%|██████████| 79/79 [50:36<00:00, 38.44s/it]\n"
     ]
    }
   ],
   "source": [
    "fihs = {}\n",
    "for key,model in MODELS.items():\n",
    "    metrics = []\n",
    "    for i in range(3):\n",
    "        met,_ = compute_fih(dl, model)\n",
    "        metrics.append(float(met))\n",
    "    fihs[key] = metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d060b8d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['VAE', 'WGANGP', 'Diff-UNet'])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MODELS.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c9bc72d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(PATH_OUTPUT,\"fihs.json\"),\"w\") as f:\n",
    "    json.dump(fihs,f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a615441",
   "metadata": {},
   "outputs": [],
   "source": [
    "records = {(key,float(np.mean(values)),float(np.std(values))) for key,values in fihs.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "509577b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "VAE.load_from_checkpoint(os.path.join(PATH_MODEL,\"VAE\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cc21b15",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "imaging",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
